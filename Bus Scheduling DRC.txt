This DRC is about ways that we can schedule the processing of messages coming over RabbitMQ. It has a particular eye to how the GIMP works as that has some particular requirements that most other apps don't, but is still relevant outside of that.

PREAMBLE:

We encountered a situation with the Ouigo system utilising components on the bus resulting in simple pnr retrieve messages for the high-priority, "real-time" ticket/barcode services becoming stuck behind the large and complex manifest retrieve messages for survey sends - a long running, low priority batch job, which would make tickets unavailable for about 10 minutes each day. Additionally the odd pnr retrieve message was caught behind smaller manifest retrievens for the train status service - a long-ish running, high-ish priority job which had far lower impact on ticketing than the survey sends, but is still undesirable when it happens as regularly as it did. The ticketing service will timeout requests if it takes more than a minute, and this service is called externally it means that the problem is highly visible outside of 15below, both to our client and to travellers using the barcode service for their mobile apps.

This illustrated the fact that we have 2 independent concepts that can effect how we want to handle work - priority and job size, and that these are both on a continuum rather that being purely binary values.

As a quick solution to their immediate production issues we were able to set up a second rabbit virtual host to move all of the longer running jobs to, but having separate streams for each job that we need to run isn't a scalable solution for long term improvements to PASNGR as we look to make more use of the bus, and these issues may have some impact on zorillo as well, so we need to have a good, consistent approach.


MORE TECHNICAL DETAIL:

The Ouigo problem saw a gimp instance take on a manifest retrieve for all of the previous day's travellers, which unsurprisingly takes a long time to complete. Whilst this is running, none of the quick PNR retreive for the ticketing system will be processed, so things time out. A quick fix of running multiple gimp instances was suggested to alleviate this problem. By default EasyNetQ will have a prefetch count of 50 so some messages could still get caught behind the long job, but even if this were set to 1 the architecture of the gimp is such that it has multiple internal processes on different threads, and its own internal queuing mechanism. This means it quite happily pulled new PNR requests off the bus as fast as they came in, but they aren't processed until long after, so even for this simple case we need to look to rearchitect the way that some of our core components work - this may lead to having 2 distinct gimps, one old school version, and a new pure bus based one that works differently. The internal queuing is probably important for coping with the number of connections that we can have open to a GDS at a given time, so we may need cunning around such things. Even if the gimp lost its internal queue and completed its work before going back to the message bus, if all gimp instances are set to handle all message types then a number of batch jobs happening in the same time frame could still lead to such issues, so we need ways of limiting and prioritising what will be processed.

Note: even with a prefetch of 1, as different messages have different queues we can still have one message per subscription become stuck behind a batch job - this is far less of a problem, but not suitable for our long term aims, and can add up to a number of queueing messages if the service handles a variety of requests.

Upon completion of the manifest retrieval, a saga decides who to send survey messages to (based on percentages, only sending to outbound journies so people don't get it twice, etc.) and then pushes out render requeust messages in a very quick, lightweight process, and the trainstatus job has a very similar design. This means that if we have ways of coping with the work load at the GDS we will then flood the renderer queues with thousands of low priority messages, albeit each individually being much quicker ones than the retreive step, so we need to continue to be intelligent with the way that we handle all the steps in a workflow.

**** With something like the gimp functionality we only have a limited number of connections that a client needs to share efficiently, so we need to have an extra degree of intelligence here. With renderers or other workflowy services etc we can have different instances on different boxes subscribing to different messages or topics, but we can't just have multiple gimps side by side configured to run different types of job and assume that they will run efficiently independently of each other.


INTERESTING(ISH) RABBIT SCENARIOS:

To try and demonstrate the different ways that we could use EasyNetQ I've created a small demo suite of a message sender and a receiver which lets us subscribe with a variety of techniques, and send messages with topics for priority and length. Obviously Rabbit & EasyNetQ aren't interested in these ideas of high or low priority, and short or long running, so we have to try to code the sematics we want around them.

*1 IBus with multiple subscriptions*
** high & low sub, prefetch of 1 - interleave requests on the same thread
** high & low sub, prefetch > 1 - Will interleave requests as they arrive, but will single-mindedly work through prefetch queues (possibly in the order of registration) if work is waiting when starting up
*** 2 high subs & 1 low sub, prefetch = 50 will mean that they interleave 2:1 when work is queued at startup (in chunks of 50), but not when a batch is dumped on the bus in a big hit. However with a 50 prefetch and a LOT on the queue it will occassionally drain the prefetch meaning that the more subscribed one clears faster but not in an entirely predictable manner.
*** 2 high subs & 1 low sub, prefetch = 1 will mean that they interleave 2:1 on incoming work.
** of course - l long running process holds everything behind it up.

* >1 IBus instance - same rabbit host
** subs to different bus instances are handled by different threads so work can be handled simultaneously for different streams. this is a nice easy way to get some thready handling in for free.

* Unsubscribing from messages
** The current implementation of easynetq doesn't allow unsubscribing from a message, so it's not possible to simply stop accepting high priority messages on the queue, and even with a prefetch setting of 1, single messages will get stuck behind long running ones which we can't allow for a full solution! Disposing a queue will push anything in its prefetch back to the rabbit server so can be used to unhook from _all_ subscriptions, but doing this in a subscription handler makes it return the message it's currently processing as well, so it'll get into an endless loop of running the handler for the same message but never removing it from the queue (and potentially screwing things up if were not idempotent). By setting up 2 different queues, and some thread locking, the non blocking subscriber simulates what might happen if EasyNetQ was extended to allow unsubbing. This implementation will drop its high priority subscription when processing a long running job, and restart them once it has completed it. This would allow us to have multiple instances of an app with differing config so that some wouldn't touch long running jobs, and others would take them on when free, but wouldn't keep hold of any if they're processing something more chuntery, so we could scale out, but be sure about not batchjobbing ourselves to death. This setup illustrates the concepts of high & low priority as well as long and short running all in action mixed up together.
This needs other subscribers running alongside it to see it have effect. This could be a different bus on the same app, or click spawn reciever to spin out another instance which makes it a bit more obvious who's doing what.

* Async Subscription
** Using SubscribeAsync in EasyNetQ allows multiple messages to be handled simultaneously on different threads. It appears that this usage is handled dynamically using the thread pool or similar. This is good for processes where we're not that concerned about handling access to resources in a very controlled manner, but may not be ideal for coping with a GDS type scenario.
Piling in a bunch of requests to different async subscribers doesn't have the same ordered alternation of handling, but averages out over a batch. As with non-async, multiple subs on the same bus with a big prefetch has a lumpy approach to biasing which messages are handled, but prefetch of 1 smoothes this out.
Async subscribes will allow a large number of long running jobs hitting the bus quickly to flood the threadpool if we have a big prefetch queue, so sharing a sub can still lead to high priority work getting stuck. Running another bus instance with non-async stuff can help keep them flowing if different buses subscribe to different things. Note - different bus instances with async subs share the same threadpool, so can still overwhelm stuff unlike the non-async.
The app will scale the threadpool over a few loadings, so early dumps of large numbers won't see many simultaneous threads, so push a few runs of 50 for it to level out.
We can control biasing of subscriptions to some extent by having multiple async subs to the same queue and a prefetch of 1



------------ NOTE - as mentioned, the GDS stuff is a bit special casey - a client only has X simultaneous connections that they can use, so we can't just rely on being able to spin up more instances, especially if we have gds interactors that can use multiple connections for heavy loads, as we can still get jobs being blocked - we want to manage these things well!
-How do the main GDSes work in terms of call durations - are they all _pretty_ quick? could a manifest retreive then punt pnr requests back to the bus with low priority? and similar approaches for other ponderous jobs? If all native gds processes could be relatively quick calls (network conditions permitting) some of which go on to need a number of other calls, this requeuing with a low priority may help make it more efficient. Currently we spin up GDS objects in the gimp as it processes each request - i think that these interact with a pool of connections in some way as the logs show us connecting and disconnecting every few minutes of inactivity, but there is some complexity here. if we have 1 sub with synchronous subscriptions that do the full job (as we want for a proper bus based interface) then it'll only use 1 newskies connection at a time, and queue requests behind each other if they pile in. Having a bus subscription per connection, all handling high priority requests, some (possibly all) handling low as well - possibly with a high:low subscription ratio on each bus will mean that we can get more small requests through side by side over the wire easily. We also have the option of using 1 bus with async subscriptions, but this loses some control over the threading, and managing those GDS connection resources is vital here. We'll need to balance having multiple gds interactors with the number of available GDS connections, so different instances of the service for CPU scaling or redundancy - and maybe old Gimps - can coexist - ideally some sort of heartbeaty communication mechanism could allow them to increase or decrease bus subs as other gds interactors come on and off line.
All that said, we may want a pool of gds connections that we could block threads waiting for em to free up as we need em, then release them once we've hit the system. This would mean that other things that we need to do in the process (xslt or some other form of translation, local DB interaction if we decide to keep doing this) wouldn't be blocking the connections.
***NOTE IN A NOTE NOTE***
having 1 long running process that does a bunch of stuff internally makes it easy to know when it's finished - especially for things like pushing a newly created data source on to the next process - if we break this into getting lists then getting the PNRs we'd need to have something keep track of all the sub tasks and handle the job completion when all have returned successfully (not just the last one created, as earlier ones may be held up) and would need to be able to handle failures with timeouts or something.
***OTHER NOTE IN A NOTE NOTE***
if we're rethinking gimpy behaviour in a bus based world it gives us a chance to consider other design decisions. existing customers already have mountains of xslt, so ditching this altogether might be unrealistic (especially if we're still running the oldschool gimp alongside for some things) but we could look into having optional code based translation with LINQ to XML or somat. We may also want to remove or reduce the database involvement, just pushing out the data in the completed message and let client apps decide whether they want to push this to a DB or not.

HOW DO WE (ESPECIALLY MIKE HADLOW) FEEL ABOUT HAVING MULTIPLE BUS CONNECTIONS RATHER THAN SHARING PER SUB? seems like a good way to handle threading to me, _if_ we are doing it purposefully by design.


If we want the bus to provide a scaling mechanism it seems like a good idea to default to using a prefetch of 1, and opt in to increasing it if we have need, rather than using the rabbity default of 50. Prefetch is probably useful when message processing times are so trivial that network latency interferes with performance, but that doesn't seem to be the case for most of our processes. Also it is currently set at the connection level. This might be better suited to the subscription itself for better balancing of different jobs sharing the same connection, although i've not been able to experiment with this to see how rabbit punts stuff to us so it may not be that useful.

For processes that run through a number of services, we will tend to want to push around concepts of priority or batchiness (along with some sort of job indetifier) so it doesn't just bottleneck further down the queue. Rabbit doesn't have a built in mechanism for telling us about routing keys and whatnot that might be controlling this. When we add a subscriber we can write arbitrary code, so could push this info into the handler and let it then propogate it as appropriate when pushing out its results, but we might want something a bit more prescriptive - Rabbit (or 15b variant/wrapper thereof) API change? Base object for messages?
